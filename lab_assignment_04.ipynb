{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samhithaPeddireddy/Samhitha_Info5502_Spring2022/blob/main/lab_assignment_04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpYGQvhY3o31"
      },
      "source": [
        "## The fourth Lab-assignment (02/17/2022, 50 points in total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6P2V4RVE3o32"
      },
      "source": [
        "The purpose of this exercise is to understand Exploratory Data Analysis, we will practice the EDA by working on multiple datasets which have different characteristics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSB5dfAs3o33"
      },
      "source": [
        "Question 1 (15 points). Understand a text corpus for domain-specific text classification. Download the dataset from the following link: https://osf.io/8mjcy/ . Conduct EDA of the dataset from the following aspects:\n",
        "\n",
        "(1) How many categories in total?\n",
        "\n",
        "(2) How many documents under each category?\n",
        "\n",
        "(3) What is the avergae number of sentence (on average)?\n",
        "\n",
        "(4) What is the average number of words (on average)?\n",
        "\n",
        "(5) Visualize the top 50 terms and rank the terms by frquency (remove stop words first: https://gist.github.com/sebleier/554280)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/preprocessed_cases[cases_29404].zip"
      ],
      "metadata": {
        "id": "sxaQgiVi3vK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51Io8gQ23o33"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Your answer here (code + explanation):\n",
        "\n",
        "import os\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import PlaintextCorpusReader\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger') \n",
        "\n",
        "#1)\n",
        "cat_path = os.listdir(\"/content/preprocessed_cases[cases_29404]\")\n",
        "if(\".DS_Store\" in cat_path):\n",
        "  cat_path1=cat_path.remove(\".DS_Store\")\n",
        "no_cat = len(cat_path)\n",
        "#print(cat_path)\n",
        "print('{} categories are present'.format(no_cat))\n",
        "\n",
        "#2)\n",
        "for i in cat_path:\n",
        "    docs = os.listdir(\"/content/preprocessed_cases[cases_29404]/{}\".format((i)))\n",
        "    print('In file {} we have {} documents'.format(i,len(docs)))\n",
        "\n",
        "#3 and 4\n",
        "\n",
        "scount=0\n",
        "scount1=0\n",
        "wcount=0\n",
        "wcount1=0\n",
        "corpus_root = '/content/preprocessed_cases[cases_29404]'\n",
        "corpus = PlaintextCorpusReader(corpus_root, '.*txt')\n",
        "id_list = corpus.fileids()\n",
        "len_total=len(id_list)\n",
        "print(len_total)\n",
        "for i in id_list:\n",
        " r=open(\"/content/preprocessed_cases[cases_29404]/{}\".format(i),\"rt\")\n",
        " data = r.read()\n",
        " print(\"/content/preprocessed_cases[cases_29404]/{}\".format(i))\n",
        " scount=scount+len(data.split(\".\"))\n",
        " wcount=wcount+len(data.split())\n",
        "print(scount)\n",
        "print(wcount)\n",
        "print('Average number of sentences is {}'.format(scount/len(id_list)))\n",
        "print('Average number of words is {}'.format(wcount/len(id_list)))\n",
        "\n",
        "for i in id_list:\n",
        " r1=open(\"/content/preprocessed_cases[cases_29404]/{}\".format(i),\"rt\")\n",
        " data1 = r1.read()\n",
        " scount1=scount1+len(sent_tokenize(data1))\n",
        " wcount1=wcount1+len(word_tokenize(data1))\n",
        "print(scount1) \n",
        "print(wcount1)\n",
        "print('Average number of sentences is {}'.format(scount1/len(id_list)))\n",
        "print('Average number of words is {}'.format(wcount1/len(id_list)))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.probability import FreqDist\n",
        "\n",
        "corpus_root = '/content/preprocessed_cases[cases_29404]'\n",
        "corpus = PlaintextCorpusReader(corpus_root, '.*txt')\n",
        "id_list = corpus.fileids()\n",
        "for i in id_list:\n",
        " r1=open(\"/content/preprocessed_cases[cases_29404]/{}\".format(i),\"rt\")\n",
        " data1 = r1.read()\n",
        " stopwords = nltk.corpus.stopwords.words('english')\n",
        " #text_remstopwrds = [word for word in data1 if word not in stopwords]\n",
        " fdist1 = FreqDist(word1.lower() for word1 in word_tokenize(data1) if word1 not in stopwords)\n",
        " #print(fdist1)\n",
        "fdist1.most_common(50)\n",
        "fdist1.plot(50, cumulative=True)\n",
        "#sent = 'This is an example sentence an an the is is is is an an example'\n",
        "#fdist1 = FreqDist(word.lower() for word in word_tokenize(sent))\n",
        "#print(fdist1)\n",
        "#fdist1.most_common(3)\n"
      ],
      "metadata": {
        "id": "GjR91xTRQHq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrMDWAYa3o34"
      },
      "source": [
        "Question 2 (20 points). Continue practicing how to do simple exploratory data analysis with Python. The dataset that has been used in this question is called Student Academics Performance Data Set, which can be downloaded at: https://github.com/unt-iialab/info5502-spring2022/blob/main/datasets/lab-assignment-04-student-performance.csv . Please conduct the following analysis:\n",
        "\n",
        "(1) Description of all the columns: for each colmun, count number of rows, number of unqiue element, the frequency of each unqiue element. \n",
        "\n",
        "\n",
        "(2)  Visualize the count for each value in the 'Study Hours' column. Note: Study Hours values comes from this >= 6 hours Good >= 4 hours Average < 2 hours Poor.\n",
        "\n",
        "(3) Using factorplot to show the relationship between numerical and categorical value. Please use figure to show relation between [Study Hours] and [End Semester Percentage]. Percentage is calculated based on the following:\n",
        "\n",
        "if percentage >=80 then Best\n",
        "If percentage >= 60 but less than 80 then Very Good\n",
        "If percentage >= 45 but less than 60 then Good\n",
        "If Percentage >= 30 but less than 45 then Pass\n",
        "If Percentage < 30 then Fail\n",
        "\n",
        "(4) Get the correlation from the data, then plug it to heatmap function and show it as following:\n",
        "\n",
        "![heatmap.png](attachment:heatmap.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPck3E4q3o35"
      },
      "outputs": [],
      "source": [
        "# Your answer here (code + explanation):\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unUbFbfA3o35"
      },
      "source": [
        "Question 3 (15 points). Download a dataset from Kaggle or other sources. Propose three questions that you wanna to know from the dataset, then write python code to conduct EDA analysis to answer the three questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GhK7_pr23o36",
        "outputId": "33dc8973-af28-4561-c0c7-f4beeaea0d0f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nPlease list your questions here:\\n\\n\\n\\n\\n\\n'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Your three questions:\n",
        "\n",
        "'''\n",
        "Please list your questions here:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6kJp3IK3o36"
      },
      "outputs": [],
      "source": [
        "# You code to answer the three questions (Please add comments in the code):\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "lab_assignment_04.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}